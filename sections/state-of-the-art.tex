\section{State of the art} \label{sec:body_state_of_the_art}
Evaluating output from language models has always been challenging and since the field has been moving so fast in recent years, research on this has trouble keeping up. The models are evidently becoming better but we lack of ways to accurately describe that progress. I believe this is bad and eventually may lead us to a dead end where we can't find ways to improve models because although we may still hold onto the steering wheel, we could be lost in the open sea with no orientation of where we came from and what course to take next. 

NLP tasks like summarization, question answering and machine translation have had the advantage of requiring a fairly objective output. Although there is a spectrum of possible translations, summarizations and answers to a given input the set of valid outputs is constrained. This enables us to use reference outputs as labels and so we've developed metrics that compare model predictions with the reference to evaluate the model performance. One such metric is BLEU \cite{papineni2002bleu}.  

\begin{equation}
p_n = \frac{\sum_{C \in Candidates} \sum_{\text{n-gram} \in C}\text{Count}_{clip}(\text{n-gram})}{\sum_{C' \in Candidates} \sum_{\text{n-gram}' \in C'}\text{Count}(\text{n-gram}')}
\end{equation}

The metric calculates the precision $p_n$ for the generated text (candidate) by counting n-grams that are present in the reference text and divide by the total number of n-grams from the candidate text. The maximum counts for each n-gram in the candidate however are clipped to the total number of that n-gram in the reference text. Otherwise a model could output the following "the the the the the the" for the reference text "the cat is on the mat" which would lead to a precision of $\frac{6}{6}$. With clipping however "the" will only be counted up to 2 as is the case in the reference text. So the precision will be $\frac{2}{6}$. Because the metric does not take recall into account short and precise candidates have an advantage over longer ones. Take for instance the reference "the cat is on the mat and looks at the birds" where the candidate "the cat is on the mat" would get a precision of one, while the candidate "the cat is on the mat and looks at the house" would get $\frac{10}{11}$, which results in a higher precision score even though the second candidate was closer to the reference text. The penalty is calculated as follows:
\begin{equation}
    BR = \text{min}(1, e^{1-\frac{l_{ref}}{l_{gen}}})
\end{equation}
Generated candidates that are shorter than the reference text are penalized slightly but longer ones are kept at one. This term then is multiplied by the geometric mean of different values for $n$. The authors used $N=4$.
\begin{equation}
    BLEU = BP \left(\prod_{n=1}^N p_n\right)^{\frac{1}{N}}
\end{equation}

ROUGE-N \cite{lin2004rouge} is another evaluation metric used in NLP tasks originally proposed for summarization, which focuses on recall rather than precision. It measures the overlap of n-grams between the generated text (candidate) and the reference text. The main difference from BLEU is that ROUGE-N calculates the proportion of n-grams in the reference text that are also present in the candidate text. This way, ROUGE-N encourages longer, more informative candidates, as it considers the coverage of the reference text in the generated output. Researchers found later that just concentrating on recall has can have strong negative effects. That led them to combine BLEU and ROUGE-N into one formula using the harmonic mean which strongly resembles the F-score \cite[p. 152]{tunstall2022transformers}. Since then many variations have been proposed but they are share the limitation of relying on a reference text.

Another metric commonly found in causal language modeling is perplexity, which expresses how "confused" the model is. Lower values for perplexity indicate better model performance.
\begin{equation} \label{eq:ppl}
    \text{PPL}(X) = \exp \left\{ {-\frac{1}{t}\sum_i^t \log p_\theta (x_i|x_{<i}) } \right\}
\end{equation}
Since most causal language models use cross-entropy loss and output probability distributions of log-likelihoods over the vocabulary, the conversion to perplexity is straightforward. It is worth noting that perplexity only provides a quantitative measure based on how well the model can predict the next words given an input sequence. While this is important during training from scratch, it becomes less informative for specific downstream tasks like generating mnemonics, as I hope to show later.

Some tasks are of such magnitude that training and evaluating with traditional, comparably simple methods just won't be enough. Reinforcement Learning from Human Feedback (RLHF) was one of the cornerstones behind the incredible capabilities of Chat-GPT \cite{lambert2022rlhf}. The process involves three higher-level steps. To begin with, a pretrained language model is needed. In the case of Instruct-GPT, a precursor to Chat-GPT a smaller version of GPT-3 was used \cite{lambert2022rlhf}. Following, a reward model is trained that typically takes in two sequences as its input and outputs a scalar, indicating the quality of the generated text. The labels for this model are obtained from humans. Rather than giving a score for each sequence, participants are asked to rank the given sequences. This avoid high variance among participant's feedback. In the last step for each iteration of the training process the reward model scores are obtained from two generated model outputs. One from the inital LM and one from the current fine-tuned iteration. Based on the difference between the two scores a penalty is calculated to avoid rapid shifts between the models. Finally the resulting reward is used to update model weights. Evidently, this has been a very successful approach in improving capabilities of language models. Referring back to the application I built in conjunction with this paper Figure \ref{figure:robanohashi_example}, users can up and down vote mnemonics as well as adding new ones. If sufficient people would use and contribute to the application, the data would then be inherently ranked and could be applied directly to train a reward model used to fine-tune prompts or entire models. Despite it's success RLHF has limitations. One problem we face today is that models like ChatGPT tend to hallucinate rather than giving an "honest" reply, namely not knowing the correct answer. This could be rooted in the inability of humans to distinguish between good and bad model outputs as Rob Miles has pointed out \cite{miles_chatgpt}. During the training of the reward model the human is likely to prefer a factually wrong answer if he doesn't know better and the second option merely states some form of "I don't know". This in turn incentivizes the model to deceive the human. Another study \cite{clark2020human} evaluated human evaluation on distinguishing between human and model-generated text. They found that participants weren't able to distinguish between the two classes. As it turned out most participants focused on surface-level text qualities rather than the actual content of the text to make a decision. This paper was published before Chat-GPT was unleashed. Consequently a lot of the bias towards the quality of language models will likely be eradicated. Nonetheless this raises significant concerns about our ability to further train models with human feedback because teaching the participants to distinguish between the classes also did not raise accuracy significantly.

As of now I lack the resources to evaluate mnemonics with RLHF. In addition metrics like BLEU and ROUGE are too simplistic and also constrain the models creativity which has been highlighted by the authors of DeepMnemonic \cite{deepMnemonic2022}, a password mnemonic generation model that takes in randomly generated passwords and uses them as acronyms to build mnemonic sentences. To mitigate the shortcomings of BLEU they conducted a user study to further evaluate their model. Moreover they integrated a simple metric that counts how many of the words in the generated mnemonic sentence start with the letter of the respective acronym. 
In a similar way research was done on evaluating generated song lyrics from different models by conducting a user study as well as developing an automatic metric that focused on key elements of good lyrics \cite{lyrics_gen_2021}. They considered the quality of the lyrics by counting lines, words and syllables. Secondly rhyme density was taken into account and lastly they inferred the sentiment of the generated text, as emotional lyrics are generally considered to be better. Interestingly, their automatic metric had high correlation with the user study results. Based on that success I will take a similar approach, that measures key elements of good mnemonics to assess the generated output. 
