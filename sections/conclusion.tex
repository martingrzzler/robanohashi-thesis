\section{Conclusion} \label{sec:conclusion}
In conclusion I investigated various ways to evaluate keyword mnemonics in the context of Japanese Kanji. Numerous studies had suggested that linguistic features like vividness enhance recall of the learning material. Additionally, it had been shown that language bizarreness also increases recall. 

Based on this research I investigated strategies to assess vividness and bizarreness of four different groups of mnemonics, differing in quality. All experiments failed to significantly separate the groups. This was likely due to the simplistic nature of the experiments as they examined the mnemonics in their vividness on a per word basis. Further research on how the aforementioned linguistic features are manifested in language would be necessary to approximate them with quantitative metrics.

Since bizarre language tends to be rare I used perplexity measures from GPT-2 to approximate bizarreness. I expected the better mnemonics to have higher perplexity than the lower ones. The results however were inconsistent with my expectations indicating a bias towards generated text over the mnemonics created by humans. To approximate bizarreness accurately further research is necessary. Since there exists no dataset with labels that describe bizarrenes one could use LLMs to do the annotation task and then train a model for classification. This approach has worked well with classification tasks in the past \cite{ding2022gpt3}. 

I conducted additional experiments inspired by the obvious observation that the input words to the mnemonic, i.e. the radical meanings and the kanji meaning, should be well represented and strongly connected in the mnemonic as a whole. This is essentially what keyword/keyphrase extraction algorithms aim to do. Following I compared different such algorithms in their ability to extract the input words from the mnemonic. All tested algorithms significantly separated the evaluation data. However, graph-based models outperformed statistical ones. I suspected that this may be due to fact that graphs are better at capturing semantic relationships between words and phrases than statistical methods that largely depend on features like word frequencies to extract keyphrases. 

Given the successful results of the keyphrase based method, I reconsidered the approach of examining the language vividness limited to keyphrases that do not contain input words to the mnemonic. This approach also did not separate the groups significantly.

LMMs are becoming increasingly sophisticated and can do more useful things than what would be commonly associated with predicting the next token. Hence I was curious to examine the ability of GPT-3 to distinguish good from bad mnemonics. The model was able to separate between groups but with a bias towards mnemonics generated by itself while often rejecting examples from the best group. Even larger models like GPT-4 could potentially show even better results which would question the utility of using a metric in the first place. This remains to be seen however.

For now I have proposed a simple recall-based metric that can be used to choose optimal prompts for LMMs or to evaluate fine-tuned models dedicated to generating mnemonics. Since the metric was exclusively tested in the domain of learning Japanese Kanjis, it may or may not translate well to other domains where keyword mnemonics are used. With Kanjis the input words are given by the radical components of it. Usually the keywords aren't found that easily. E.g. English native students learning a second language like Spanish, first have to find appropriate keywords that sound similar to the foreign word. Evaluating the phonetic similarity between keyword and foreign word could be more important here. Nonetheless, my proposed method could then be used to evaluate the resulting mnemonic once the input words are found.   

