\section*{Abstract}

The increasing capability of AI language models has opened up the possibility of generating keyword mnemonics to enhance learning efficiency. This study explores methods to evaluate such mnemonics in the context of learning Japanese Kanji. Previous research indicated that vivid and bizarre language positively affect memory retention; however, this study found that strategies based on these features failed to significantly separate mnemonic groups of varying quality.

Further experiments focused on keyword representation within generated mnemonics, using keyword extraction algorithms to effectively distinguish between groups. The performance of GPT-3 in evaluating mnemonics revealed a bias towards self-generated examples while often dismissing high-quality mnemonics. A recall-based metric was proposed to optimize prompts for language models or evaluate models specifically tailored for mnemonic generation.

Although tested exclusively with Japanese Kanji, the metric may have broader applicability in other domains, such as second language learning. Future research should investigate the potential of larger models, such as GPT-4, in evaluating mnemonics and further refining the proposed metric.
